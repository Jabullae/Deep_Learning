import numpy as np
import matplotlib.pyplot as plt
from matplotlib import cm
from mpl_toolkits.mplot3d import Axes3D

# 目标函数
def f(x,y):
    return x**2 + y**2 + x*y

# 对x求偏导
def partial_x(x,y):
    return 2*x+y
    
# 对y求偏导
def partial_y(x,y):
    return 2*y+x

X = np.arange(-10, 10, 1)
Y = np.arange(-10, 10, 1)

X, Y = np.meshgrid(X, Y)
Z = f(X,Y)

# 绘制曲面
fig = plt.figure()
ax = Axes3D(fig)
surf = ax.plot_wireframe(X, Y, Z, rstride=1, cstride=1, cmap=cm.viridis)
#ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=cm.viridis)
ax.set_xlabel('x')
ax.set_ylabel('y')
ax.set_zlabel('z')

# 随机初始点
x = 8
y = 5
z = f(x,y)
next_x = x
next_y = y

xlist=[x]
ylist=[y]
zlist=[z]

# 设定一个学习率
step = 0.01
while True:
    next_x = x-step * partial_x(x,y)
    next_y = y-step * partial_y(x,y)    
    next_z = f(next_x, next_y)
    #print(next_x,next_y,next_z)
    
    # 小于阈值时，停止下降
    if z - next_z < 1e-9:
        break
        
    x = next_x
    y = next_y
    z = f(x,y)
    xlist.append(x)
    ylist.append(y)
    zlist.append(z)

ax.plot(xlist,ylist,zlist,'r--')
plt.show()


import numpy as np


def numerical_diff(f, x):
    h = 10e-50
    return (f(x + h) - f(x)) / h


np.float32(1e-50)


def numerical_diff(f, x):
    h = 1e-4    # 0.0001
    return (f(x+h) - f(x-h)) / (2*h)


def function_2(x):
    return x[0]**2 + x[1]**2


def function_tmp1(x0):
    return x0*x0 + 4.0**2.0
numerical_diff(function_tmp1, 3.0) 


def function_tmp2(x1):
    return 3.0**2.0 + x1*x1
numerical_diff(function_tmp2, 4.0)  


def _numerical_gradient_no_batch(f, x):
    h = 1e-4 # 0.0001
    grad = np.zeros_like(x) # x와 형상이 같은 배열을 생성

    for idx in range(x.size):
        tmp_val = x[idx]

        # f(x+h) 계산
        x[idx] = float(tmp_val) + h
        fxh1 = f(x)

        # f(x-h) 계산
        x[idx] = tmp_val - h 
        fxh2 = f(x) 

        grad[idx] = (fxh1 - fxh2) / (2*h)
        x[idx] = tmp_val # 값 복원

    return grad


def numerical_gradient(f, X):
    if X.ndim == 1:
        return _numerical_gradient_no_batch(f, X)
    else:
        grad = np.zeros_like(X)
        
        for idx, x in enumerate(X):
            grad[idx] = _numerical_gradient_no_batch(f, x)
        
        return grad


print(_numerical_gradient_no_batch(function_2, np.array([3.0, 4.0])))
print(_numerical_gradient_no_batch(function_2, np.array([0.0, 2.0])))
print(_numerical_gradient_no_batch(function_2, np.array([3.0, 0.0])))


def gradient_descent(f, init_x, lr=0.01, step_num=100):
    x = init_x
    x_history = []

    for i in range(step_num):
        x_history.append( x.copy() )

        grad = _numerical_gradient_no_batch(f, x)
        x -= lr * grad

    return x, np.array(x_history)


def function_2(x):
    return x[0]**2 + x[1]**2

init_x = np.array([-3.0, 4.0])    

lr = 0.1
step_num = 20
x, x_history = gradient_descent(function_2, init_x, lr=lr, step_num=step_num)

plt.plot( [-5, 5], [0,0], '--b')
plt.plot( [0,0], [-5, 5], '--b')
plt.plot(x_history[:,0], x_history[:,1], 'o')

plt.xlim(-3.5, 3.5)
plt.ylim(-4.5, 4.5)
plt.xlabel("X0")
plt.ylabel("X1")
plt.show()


! pip install common


# coding: utf-8
import sys, os
sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정
import numpy as np


class simpleNet:
    def __init__(self):
        self.W = np.random.randn(2,3) # 정규분포로 초기화

    def predict(self, x):
        return np.dot(x, self.W)

    def loss(self, x, t):
        z = self.predict(x)
        y = softmax(z)
        loss = cross_entropy_error(y, t)

        return loss

x = np.array([0.6, 0.9])
t = np.array([0, 0, 1])

net = simpleNet()

f = lambda w: net.loss(x, t)
dW = numerical_gradient(f, net.W)

print(dW)



