import tensorflow as tf
import numpy as np


# 모두 1 또는 0인 텐서
x = tf.ones(shape = (2, 1))
print(x)

x = tf.zeros(shape = (2, 1))
print(x)


# 랜덤 텐서
x = tf.random.normal(shape = (3, 1), mean = 0, stddev = 1)
print(x)

x = tf.random.uniform(shape = (3, 1), minval = 0, maxval= 1)
print(x)


# 넘파이 배열에 값 할당하기
x = np.ones(shape = (2, 1))
x[0, 0] = 0
x


# 텐서플로 텐서에는 값을 할당하지 못한다.
x = tf.ones(shape = (2, 1))
x[0, 0] = 0.


# 텐서플로 변수 만들기
v = tf.Variable(initial_value = tf.random.normal(shape = (3, 1)))
print(v)


# 텐서플로 변수에 값 할당하기
v.assign(tf.ones((3, 1)))


# 변수 일부에 값 할당하기
v[0, 0].assign(3.)


v.assign_add(tf.ones((3, 1)))


# 기본적인 수학 연산
a = tf.ones((2, 2))
b = tf.square(a)
c = tf.sqrt(a)
d = b + c
e = tf.matmul(a, b)
e += d
print(a)
print(b)
print(c)
print(d)
print(e)


#GradientTape 사용하기
input_var = tf.Variable(initial_value=3.)
with tf.GradientTape() as tape:
    result = tf.square(input_var)
gradient = tape.gradient(result, input_var)
gradient


# 상수 텐서 입력과 함꼐 GradientTape 사용하기
input_const = tf.constant(3.)
with tf.GradientTape() as tape:
    tape.watch(input_const)
    result = tf.square(input_const)
gradient = tape.gradient(result, input_const)
gradient


# GradientTape를 중첩하여 이계도 Gradient 계산하기
time = tf.Variable(0.)
with tf.GradientTape() as outer_tape:
    with tf.GradientTape() as inner_tape:
        position = 4.9 * time ** 2
    speed = inner_tape.gradient(position, time)
accelaration = outer_tape.gradient(speed, time)
print(speed)
print(accelaration)


# 2D 평면에 두 클래스의 랜덤한 포인트 생성하기
num_samples_per_class = 1000
negative_samples = np.random.multivariate_normal(
    mean = [0, 3],
    cov = [[1, 0.5], [0.5, 1]],
    size = num_samples_per_class)
positive_samples = np.random.multivariate_normal(
    mean = [3, 0],
    cov = [[1, 0.5], [0.5, 1]],
    size = num_samples_per_class)
print(negative_samples.shape, positive_samples.shape)


# 두 클래스를 (2000, 2) 크기의 한 배열로 쌓기
inputs = np.vstack((negative_samples, positive_samples)).astype(np.float32)
inputs.shape


# (0과 1로 구성된) 타깃 생성하기
targets = np.vstack((np.zeros((num_samples_per_class, 1), dtype="float32"), 
                    np.ones((num_samples_per_class, 1), dtype = "float32")))
targets


# 두 클래스의 포인트를 그래프로 그리기
import matplotlib.pyplot as plt

plt.scatter(inputs[:, 0], inputs[:, 1], c = targets[:, 0])
plt.show()


# 선형 분류기의 변수 만들기
input_dim = 2
output_dim = 1
W = tf.Variable(initial_value = tf.random.uniform(shape = (input_dim, output_dim)))
b = tf.Variable(initial_value= tf.zeros(shape = (output_dim, )))


# 정방향 패스 함수
def model(inputs):
    return tf.matmul(inputs, W) + b

# 평균 제곱 오차 손실 함수
def square_loss(targets, predictions):
    per_sample_losses = tf.square(targets - predictions)
    return tf.reduce_mean(per_sample_losses)

# 훈련 스텝 함수
learning_rate = 0.1

def training_step(inputs, targets):
    with tf.GradientTape() as tape:
        predictions = model(inputs)
        loss = square_loss(targets, predictions)
    grad_loss_wrt_W, grad_loss_wrt_b = tape.gradient(loss, [W, b])
    W.assign_sub(grad_loss_wrt_W * learning_rate) # W.assign_sub = W - grad_loss_wrt_W * learning_rate
    b.assign_sub(grad_loss_wrt_b * learning_rate) # b.assign_sub = W - grad_loss_wrt_b * learning_rate
    return loss


# 배치 훈련 루프
for step in range(40):
    loss = training_step(inputs, targets)
    print(f'{step}번째 스텝의 손실 : {loss:.4f}')


# 예측한 타겟을 시각화
predictions = model(inputs)
plt.scatter(inputs[:, 0], inputs[:, 1], c = predictions[:, 0]  > 0.5)
plt.show()


W


b


x = np.linspace(-4, 6, 100)
y = -W[0] / W[1] * x + (0.5 - b) / W[1]
plt.plot(x, y, '-r')
plt.scatter(inputs[:, 0], inputs[:, 1], c = predictions[:, 0] > 0.5)
plt.show()


# Layer의 서브클래스(subclass)로 구현한 Dense층
from tensorflow import keras

class SimpleDense(keras.layers.Layer):
    def __init__(self, units, activation = None):
        super().__init__()
        self.units = units
        self.activation = activation
        
    def build(self, input_shape):
        input_dim = input_shape[-1]
        self.W = self.add_weight(shape = (input_dim, self.units),
                                initializer='random_normal')
        self.b = self.add_weight(shape = (self.units, ),
                                initializer = 'zeros')
        
    def call(self, inputs):
        y = tf.matmul(inputs, self.W) + self.b
        if self.activation is not None:
            y = self.activation(y)
        return y


my_dense = SimpleDense(units = 32, activation = tf.nn.relu)
input_tensor = tf.ones(shape = (2, 784))
output_tensor = my_dense(input_tensor)
print(output_tensor.shape)


output_tensor


from tensorflow.keras import layers
layer = layers.Dense(32, activation='relu')
layer


from tensorflow.keras import models

model = models.Sequential([
    layers.Dense(32, activation = 'relu'),
    layers.Dense(32)
])




class NaiveDense:
    def __init__(self, input_size, output_size, activation):
        self.activation = activation
        
        w_shape = (input_size, output_size)
        w_initial_value = tf.random.uniform(w_shape, minval=0, maxval=1e-1)
        self.W = tf.Variable(w_initial_value)
        
        b_shape = (output_size, )
        b_initial_value = tf.zeros(b_shape)
        self.b = tf.Variable(b_initial_value)
        
    def __call__(self, inputs):
        return self.activation(tf.matmul(inputs, self.W) + self.b)

    @property
    def weights(self):
        return [self.W, self.b]
    
    
class NaiveSequential:
    def __init__(self, layers):
        self.layers = layers
        
    def __call__(self, inputs):
        x = inputs
        for layer in self.layers:
            x = layer(x)
        return x
    
    @property
    def weights(self):
        weights = []
        for layer in self.layers:
            weights += layer.weights
        return weights


# 동적으로 만들어지지 않을때에는 입력과 출력 size를 맞춰줘야 한다. 
model = NaiveSequential([
    NaiveDense(input_size = 784, output_size = 32, activation = 'relu'),
    NaiveDense(input_size = 32, output_size = 64, activation = 'relu'),
    NaiveDense(input_size = 64, output_size = 32, activation = 'relu'),
    NaiveDense(input_size = 32, output_size = 10, activation = 'softmax'),
])


# 동적으로 층이 만들어질때에는 output과 input을 맞춰줄 필요가 없다. 
# 그 이유는 SimpleDense 클래스에 build함수가 있기 때문.
model = keras.Sequential([
    SimpleDense(32, activation = 'relu'),
    SimpleDense(64, activation = 'relu'),
    SimpleDense(32, activation = 'relu'),
    SimpleDense(10, activation = 'softmax')
])


model = keras.Sequential([keras.layers.Dense(1)])
model.compile(optimizer = 'rmsprop',
             loss = 'mean_squared_error',
             metrics = ['accuracy'])


history = model.fit(inputs, targets, epochs=5, batch_size = 128)


history.history


model = keras.Sequential([keras.layers.Dense(1)])
model.compile(optimizer = keras.optimizers.RMSprop(learning_rate = 0.1),
              loss = keras.losses.MeanSquaredError(),
              metrics = [keras.metrics.BinaryAccuracy()])

indices_permutation = np.random.permutation(len(inputs))
shuffled_inputs = inputs[indices_permutation]
shuffled_targets = targets[indices_permutation]

num_validation_samples = int(0.3 * len(inputs))
val_inputs = shuffled_inputs[:num_validation_samples]
val_targets = shuffled_targets[:num_validation_samples]
training_inputs = shuffled_inputs[num_validation_samples:]
training_targets = shuffled_targets[num_validation_samples:]
model.fit(
    training_inputs, training_targets,
    epochs = 5, batch_size = 16, validation_data = (val_inputs, val_targets))


loss_and_metrics = model.evaluate(val_inputs, val_targets, batch_size = 128)


predictions = model.predict(val_inputs, batch_size = 128)
predictions[:10]
